{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (Tokenize.py, line 205)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m2910\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-09a17fdad6db>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    from Tokenize import Document\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/bastienbrunod/Ecole/tmtt/projet_tmtt/Tokenize.py\"\u001b[0;36m, line \u001b[0;32m205\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from typing import List\n",
    "from Tokenize import Document\n",
    "import numpy as np\n",
    "\n",
    "class Vectorizer:\n",
    "    \"\"\" Transform a string into a vector representation\"\"\"\n",
    "    def __init__(self, word_embedding_path: str):\n",
    "        \"\"\"\n",
    "        :param word_embedding_path: path to gensim embedding file\n",
    "        \"\"\"\n",
    "        self.word_embeddings = KeyedVectors.load_word2vec_format(word_embedding_path, binary=False)\n",
    "        self.pos2index = {'$': 0, '\\'\\'': 1, '(': 2, ')': 3, ',': 4, '--': 5, '.': 6, ':': 7, 'CC': 8, 'CD': 9,\n",
    "                          'DT': 10, 'EX': 11, 'FW': 12, 'IN': 13, 'JJ': 14, 'JJR': 15,\n",
    "                          'JJS': 16, 'LS': 17, 'MD': 18, 'NN': 19, 'NNP': 20, 'NNPS': 21,\n",
    "                          'NNS': 22, 'PDT': 23, 'POS': 24, 'PRP': 25, 'PRP$': 26, 'RB': 27, 'RBR': 28, 'RBS': 29,\n",
    "                          'RP': 30, 'SYM': 31, 'TO': 32, 'UH': 33, 'VB': 34, 'VBD': 35,\n",
    "                          'VBG': 36, 'VBN': 37, 'VBP': 38, 'VBZ': 39, 'WDT': 40, 'WP': 41, 'WP$': 42, 'WRB': 43,\n",
    "                          '``': 44}\n",
    "        self.shape2index = {'NL': 0, 'NUMBER': 1, 'SPECIAL': 2, 'ALL-CAPS': 3, '1ST-CAP': 4, 'LOWER': 5, 'MISC': 6}\n",
    "        self.labels = ['O', 'PER', 'LOC', 'ORG', 'MISC']\n",
    "        self.labels2index = {'O': 0, 'PER': 1, 'I-PER': 1, 'B-PER': 1, 'LOC': 2, 'I-LOC': 2, 'B-LOc': 2, 'ORG': 3,\n",
    "                             'I-ORG': 3, 'B-ORG': 3, 'MISC': 4, 'I-MISC': 4, 'B-MISC': 4}\n",
    "     \n",
    "\n",
    "    def encode_features(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        Creates a feature matrix for all documents in the sample list\n",
    "        :param documents: list of all samples as document objects\n",
    "        :return: lists of numpy arrays for word, pos and shape features. \n",
    "                 Each item in the list is a sentence, i.e. a list of indices (one per token)\n",
    "        \"\"\"\n",
    "        # Loop over documents\n",
    "        \n",
    "        words=[]\n",
    "        shapes=[]\n",
    "        for doc in documents:\n",
    "        #    Loop over sentences\n",
    "            for sentence in doc.sentences:\n",
    "                words_sentence = []\n",
    "                shapes_sentence = []\n",
    "        #        Loop over tokens        \n",
    "                for token in sentence.tokens:\n",
    "                    words_sentence.append(self.word_embeddings.index2word(token.text.lower()))\n",
    "                    shapes_sentence.append(self.word_embeddings.shape2index[token.shape])\n",
    "                \n",
    "        #           Convert features to indices\n",
    "        #           Append to sentence\n",
    "                words.append(words_sentence)\n",
    "                shapes.append(shapes_sentence)\n",
    "        #   append to sentences\n",
    "        return words, shapes\n",
    "        \n",
    "    def encode_annotations(self, documents: List[Document]):\n",
    "        \"\"\"\n",
    "        Creates the Y matrix representing the annotations (or true positives) of a list of documents\n",
    "        :param documents: list of documents to be converted in annotations vector\n",
    "        :return: numpy array. Each item in the list is a sentence, i.e. a list of labels (one per token)\n",
    "        \"\"\"\n",
    "        # Loop over documents\n",
    "        labels = []\n",
    "        for doc in documents:\n",
    "        #    Loop over sentences\n",
    "            for sentence in doc.sentences:\n",
    "        #        Loop over tokens\n",
    "                labels_sentence = []\n",
    "                for token in sentence.tokens:\n",
    "        #           Convert label to numerical representation\n",
    "                    labels_sentence.append(self.word_embeddings.pos2index[token.pos])\n",
    "                labels.append(labels_sentence)\n",
    "                    \n",
    "        #           Append to sentence\n",
    "        # return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
